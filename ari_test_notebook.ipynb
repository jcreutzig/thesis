{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ee36ab",
   "metadata": {},
   "source": [
    "## Notebook for running estimates for simulated data \n",
    "\n",
    "This notebook is dedicated to a structured set of runs for \n",
    "\n",
    "(1) simulating a dynamic hierarchical DCSBM using make_hierarchical_model, \n",
    "\n",
    "(2) estimating super-/sub-community membership from the adjacency matrices using the temporally stable estimation algorithm, \n",
    "\n",
    "(3) measuring the performance of estimated membership vs ground truth, in terms of super and mean ARI \n",
    "\n",
    "Please refer to the report for details on the above.  \n",
    "\n",
    "This code assumes a default set of parameters for make_hierarchical model, and \n",
    "for a set of parameters/tuples of parameters, an exploration list.  \n",
    "\n",
    "For each exploration list, the model will cycle through the list, replacing in the \n",
    "current parameters the default value for the active parameter with the current list entry, \n",
    "simulating a model, estimating and measuring the ARIs.  This is done ten times per value, \n",
    "and mean and standard deviation are added to a dictionary.  The dictionary is saved after \n",
    "each list item is processed.  \n",
    "\n",
    "This notebook runs for quite some time; on a Dell Precision 5490 it takes about 14 hours for a complete run.  \n",
    "Reducing the number of runs from 10 to 2 helps of course, as does removing the very heavy calculations \n",
    "for n=2000 and T=50.  \n",
    "\n",
    "The procedure is smart enough to recognize and continue results of partial runs, so one can stop and re-start \n",
    "computations.  It will for each inner loop over the 10 runs re-set the random seed to the value provided, hence \n",
    "the runs will still be reproducible.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40fb14ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "#import copy\n",
    "from copy import deepcopy as dc \n",
    "\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Callable\n",
    "\n",
    "from dyn_hdcsbm import make_hierarchical_model\n",
    "from estimators_cleaned import (\n",
    "    dynamic_hierarchical_dcsbm_detection_stable, \n",
    "    dynamic_hierarchical_dcsbm_detection_simple\n",
    ")\n",
    "from ari_test import evaluate_from_hcs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c25aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter ranges and default parameters \n",
    "\n",
    "default_params = {\n",
    "    'n': 1000, 'T': 20, \n",
    "    'rho1': 0.05, 'rho2': 0.1, 'rho3': 0.4, \n",
    "    'stay_probs': [0.8, 0.8, 0.8], \n",
    "}\n",
    "default_algo_params = {'stability_weight': 0} # TODO:  remove this from the logic \n",
    "par_ranges = {\n",
    "    'varying_n': [200, 500, 1000, 1500, 2000],\n",
    "    'varying_T': [5, 10, 20, 30, 50],\n",
    "    'varying_rho_difference': [\n",
    "        (0.05, 0.10, 0.15),  \n",
    "        (0.05, 0.10, 0.25),  \n",
    "        (0.05, 0.10, 0.40),  \n",
    "        (0.05, 0.10, 0.60),  \n",
    "        (0.05, 0.10, 0.90),  \n",
    "        (0.05, 0.10, 0.95),  \n",
    "    ],\n",
    "    'varying_rho_magnitude': [\n",
    "        (0.005, 0.010, 0.040), \n",
    "        (0.010, 0.020, 0.080),  \n",
    "        (0.025, 0.050, 0.200),  \n",
    "        (0.050, 0.100, 0.400),  \n",
    "        (0.100, 0.200, 0.800),  \n",
    "    ],\n",
    "    'varying_stay_probs': [\n",
    "        [0.2, 0.2, 0.2],  \n",
    "        [0.4, 0.4, 0.4],  \n",
    "        [0.6, 0.6, 0.6],  \n",
    "        [0.8, 0.8, 0.8],  \n",
    "        [0.9, 0.9, 0.9],  \n",
    "        [0.95, 0.95, 0.95],  \n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ea3700",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help functions running experiments in loops  \n",
    "class ExperimentManager:\n",
    "    \"\"\"Manages simulation/estimation experiment with automatic saving/loading.\n",
    "\n",
    "    TODO:  Make more flexible by handing over estimator and parameter defaults/ranges.  \n",
    "        \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, output_dir: str = \"output/stabilized\", \n",
    "                 estimator: Callable = dynamic_hierarchical_dcsbm_detection_stable, \n",
    "                 random_seed=4321):\n",
    "        self.output_dir = output_dir\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        self.estimator = estimator\n",
    "        self.random_seed = random_seed\n",
    "\n",
    "    def load_results(self, filename: str) -> Dict:\n",
    "        \"\"\"Load existing results from pickle file.\"\"\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        if os.path.exists(filepath):\n",
    "            with open(filepath, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                print(f\"Loaded existing results from {filename} with {len(data) - 1} entries\")  # -1 for info\n",
    "                return data\n",
    "        else:\n",
    "            print(f\"Starting fresh results for {filename}\")\n",
    "            return {\"info\": {}}\n",
    "    \n",
    "    def save_results(self, filename: str, results: Dict) -> None:\n",
    "        \"\"\"Save results to pickle file.\"\"\"\n",
    "        with open(os.path.join(self.output_dir, filename), 'wb') as f:\n",
    "            pickle.dump(results, f)\n",
    "    \n",
    "    def update_info(\n",
    "            self, \n",
    "            results: Dict, \n",
    "            experiment_name: str, \n",
    "            defaults: Dict, \n",
    "            varying_param: str, \n",
    "            param_values: List\n",
    "        ) -> None:\n",
    "        \"\"\"Update the info entry with experiment metadata.\n",
    "        \n",
    "        This is probably suboptimal, but for now it works.  \n",
    "        \"\"\"\n",
    "        results[\"info\"] = {\n",
    "            \"experiment_name\": experiment_name,\n",
    "            \"default_parameters\": defaults,\n",
    "            \"varying_parameter\": varying_param,\n",
    "            \"parameter_values\": param_values,\n",
    "            \"last_updated\": datetime.now().isoformat(),\n",
    "            \"total_runs\": len(param_values),\n",
    "            \"completed_runs\": len(results) - 1  # -1 for info entry\n",
    "        }\n",
    "    \n",
    "    def run_single_experiment(\n",
    "            self, \n",
    "            params: Dict, \n",
    "            model_params: Dict\n",
    "        ) -> Dict:\n",
    "        \"\"\"Run a single experiment with given parameters.\"\"\"\n",
    "\n",
    "        # run model and create adjacency matrices \n",
    "        model_params_copy = dc(model_params)\n",
    "        model = make_hierarchical_model(**model_params_copy)\n",
    "        \n",
    "        adjacency_matrices = [\n",
    "            model.generate_adjacency_matrix(t) \n",
    "            for t in range(model.generator.T + 1)\n",
    "        ]\n",
    "        \n",
    "        # edge density for statistics \n",
    "        n = model_params['n']\n",
    "        edge_density = np.mean([np.sum(A)/(n*(n-1)) for A in adjacency_matrices])\n",
    "        \n",
    "        # Estimation \n",
    "        algo_params_copy = dc(params)\n",
    "        start_time = datetime.now()\n",
    "        estimation_results = self.estimator(\n",
    "            adjacency_matrices, \n",
    "            D=algo_params_copy.get('D', None), \n",
    "            verbose=False,\n",
    "        )\n",
    "        estimation_time = (datetime.now() - start_time).total_seconds()\n",
    "        \n",
    "        # Evaluate Estimation \n",
    "        results = evaluate_from_hcs(model, estimation_results)\n",
    "        \n",
    "        # Add metadata\n",
    "        results['estimation_time'] = estimation_time\n",
    "        results['edge_density'] = edge_density\n",
    "        results['model_parameters'] = model_params\n",
    "        #results['algorithm_parameters'] = params\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def print_results_summary(self, key: str, results: Dict):\n",
    "        \"\"\"Print summary of results for a single run.\"\"\"\n",
    "        print(f\"\\nResults for {key} :\")\n",
    "        print(f\"  Super-community ARI: {results['super_ari']:.3f}\")\n",
    "        print(f\"  Mean ARI: {results['mean_ari']:.3f}\")\n",
    "        print(f\"  Edge density: {results.get('edge_density', 0):.4f}\")\n",
    "        print(f\"  Estimation time: {results['estimation_time']:.1f} seconds\")\n",
    "\n",
    "\n",
    "def run_parameter_sweep(\n",
    "    experiment_name: str,\n",
    "    filename: str,\n",
    "    parameter_generator: Callable,\n",
    "    default_params: Dict,\n",
    "    varying_param_name: str,\n",
    "    manager: ExperimentManager,\n",
    "    num_runs: int = 10  # Add parameter for number of runs\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Run experiments over a list lof parameters with multiple runs per configuration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    experiment_name: str\n",
    "        Name for user display \n",
    "    filename: str\n",
    "        where to write/read pickle file \n",
    "    parameter_generator : callable\n",
    "        Generator that yields (key, model_params, algorithm_params) tuples\n",
    "        This avoids repetitive code.  \n",
    "    default_params : dict\n",
    "        Default parameters for reference\n",
    "    varying_param_name : str\n",
    "        Name of the parameter being varied.  This is used for display and info.  \n",
    "    manager : ExperimentManager\n",
    "        Experiment manager instance\n",
    "    num_runs : int\n",
    "        Number of times to run each configuration (default: 10)\n",
    "    \"\"\"\n",
    "    # Load existing results if existent  \n",
    "    results = manager.load_results(filename)\n",
    "    \n",
    "    # Get all parameter configurations\n",
    "    param_configs = list(parameter_generator())\n",
    "    param_values = [config[0] for config in param_configs]\n",
    "    \n",
    "    # Update info\n",
    "    manager.update_info(results, experiment_name, default_params, \n",
    "                       varying_param_name, param_values)\n",
    "    \n",
    "    print(\"*\"*70)\n",
    "    print(f\"EXPERIMENT: {experiment_name}\")\n",
    "    print(\"*\"*70)\n",
    "    print(f\"Default parameters: {default_params}\")\n",
    "    print(f\"Varying: {varying_param_name}\")\n",
    "    print(f\"Values: {param_values}\")\n",
    "    print(f\"Runs per config: {num_runs}\")\n",
    "    print(f\"Total configs: {len(param_configs)}\")\n",
    "    \n",
    "    # Main loop to run experiments \n",
    "    for key, model_params, algo_params in param_configs:\n",
    "\n",
    "        # Skip if already computed\n",
    "        if key in results:\n",
    "            print(f\"Skipping {key} (already computed)\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"Running experiment with {varying_param_name}={key}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        \n",
    "        try:\n",
    "            np.random.seed(manager.random_seed)\n",
    "\n",
    "            # build a list of dicts, one per run \n",
    "            run_results = []\n",
    "            for run_idx in range(num_runs):\n",
    "                print(f\"  Run {run_idx + 1}/{num_runs}...\", end='', flush=True)\n",
    "                \n",
    "                \n",
    "                # simulate and estimate.  Here we spend the majority of time.  \n",
    "                # specifically for the simple estimator, we can run into errors, \n",
    "                # and specifically for this, we try each run 3 times if needed.  \n",
    "                try:\n",
    "                    model_params_copy, algo_params_copy = dc(model_params), dc(algo_params)\n",
    "                    result = manager.run_single_experiment(\n",
    "                        algo_params_copy, model_params_copy\n",
    "                        )\n",
    "                except Exception as e:\n",
    "                    try: \n",
    "                        print(\"re-trying 1st time\")\n",
    "                        result = manager.run_single_experiment(\n",
    "                            algo_params_copy, model_params_copy\n",
    "                            )\n",
    "                    except Exception as e:\n",
    "                        print(\"re-trying 2nd time\")\n",
    "                        try: \n",
    "                            result = manager.run_single_experiment(\n",
    "                                algo_params_copy, model_params_copy \n",
    "                                )\n",
    "                        except Exception as e:\n",
    "                            print(f\"Error for {key}: {str(e)}\")\n",
    "                            raise e\n",
    "                        \n",
    "                run_results.append(result)                \n",
    "                print(f\" Super ARI: {result['super_ari']:.3f}, Mean ARI: {result['mean_ari']:.3f}\")\n",
    "\n",
    "            # collection of mean/std of relevant quantities             \n",
    "            aggregated_result = {\n",
    "                'super_ari': np.mean([r['super_ari'] for r in run_results]),\n",
    "                'super_ari_std': np.std([r['super_ari'] for r in run_results]),\n",
    "                'mean_ari': np.mean([r['mean_ari'] for r in run_results]),\n",
    "                'mean_ari_std': np.std([r['mean_ari'] for r in run_results]),\n",
    "                'estimation_time': np.mean([r['estimation_time'] for r in run_results]),\n",
    "                'estimation_time_std': np.std([r['estimation_time'] for r in run_results]),\n",
    "                'edge_density': np.mean([r.get('edge_density', 0) for r in run_results]),\n",
    "                'model_parameters': model_params,\n",
    "                'algorithm_parameters': algo_params,\n",
    "                'num_runs': num_runs,\n",
    "                'individual_runs': run_results  # Optional: keep individual results\n",
    "            }\n",
    "            \n",
    "            # Store aggregated results for this parameter, and save as pickle file \n",
    "            results[key] = aggregated_result\n",
    "            manager.save_results(filename, results)\n",
    "            \n",
    "            # Print summary\n",
    "            print(f\"\\nAGGREGATED RESULTS for {key} (over {num_runs} runs):\")\n",
    "            print(f\"  Super-community ARI: {aggregated_result['super_ari']:.3f} ± {aggregated_result['super_ari_std']:.3f}\")\n",
    "            print(f\"  Time-average total ARI: {aggregated_result['mean_ari']:.3f} ± {aggregated_result['mean_ari_std']:.3f}\")\n",
    "            print(f\"  Edge density: {aggregated_result.get('edge_density', 0):.4f}\")\n",
    "            print(f\"  Estimation time: {aggregated_result['estimation_time']:.1f} ± {aggregated_result['estimation_time_std']:.1f} seconds\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error for {key}: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    # Final save with updated info\n",
    "    manager.update_info(\n",
    "        results, experiment_name, default_params, \n",
    "        varying_param_name, param_values\n",
    "        )\n",
    "    manager.save_results(filename, results)\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(f\"EXPERIMENT COMPLETE: {experiment_name}\")\n",
    "    print(f\"Results saved to: {os.path.join(manager.output_dir, filename)}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Parameter generators for each experiment\n",
    "# One can probably do this with a generator generating function, but this works.  \n",
    "def generate_varying_n_params():\n",
    "\n",
    "    for n in par_ranges['varying_n']:\n",
    "        model_params = default_params.copy()\n",
    "        model_params['n'] = n\n",
    "        algo_params = {'stability_weight': 0}\n",
    "        yield str(n), model_params, algo_params\n",
    "\n",
    "\n",
    "def generate_varying_T_params():\n",
    "    \n",
    "    for T in par_ranges['varying_T']:\n",
    "        model_params = default_params.copy()\n",
    "        model_params['T'] = T\n",
    "        algo_params = {'stability_weight': 0}\n",
    "        yield str(T), model_params, algo_params\n",
    "\n",
    "\n",
    "def generate_varying_rho_difference_params():\n",
    "    \n",
    "    for rho1, rho2, rho3 in par_ranges['varying_rho_difference']:\n",
    "        model_params = default_params.copy()\n",
    "        model_params.update({'rho1': rho1, 'rho2': rho2, 'rho3': rho3})\n",
    "        algo_params = {'stability_weight': 0}\n",
    "        yield f\"({rho1},{rho2},{rho3})\", model_params, algo_params\n",
    "\n",
    "\n",
    "def generate_varying_rho_magnitude_params():\n",
    "\n",
    "    for rho1, rho2, rho3 in par_ranges['varying_rho_magnitude']:\n",
    "        model_params = default_params.copy()\n",
    "        model_params.update({'rho1': rho1, 'rho2': rho2, 'rho3': rho3})\n",
    "        algo_params = {'stability_weight': 0}\n",
    "        yield f\"({rho1},{rho2},{rho3})\", model_params, algo_params\n",
    "\n",
    "\n",
    "def generate_varying_stay_probs_params():\n",
    "\n",
    "    for stay_probs in par_ranges['varying_stay_probs']:\n",
    "        model_params = default_params.copy()\n",
    "        model_params['stay_probs'] = stay_probs\n",
    "        algo_params = {'stability_weight': 0}\n",
    "        yield f\"[{stay_probs[0]},{stay_probs[1]},{stay_probs[2]}]\", model_params, algo_params\n",
    "\n",
    "\n",
    "# Convenience functions for running specific experiments\n",
    "def run_all_experiments(\n",
    "        num_runs=10, \n",
    "        output_dir=\"output/tmp\",\n",
    "        estimator=dynamic_hierarchical_dcsbm_detection_stable, \n",
    "        random_seed=4321\n",
    "        ):\n",
    "    \"\"\"Run all experiments with multiple runs per configuration.\n",
    "    \n",
    "    Sweep through every parameter in the par_ranges dictionary.  \n",
    "    \"\"\"\n",
    "    manager = ExperimentManager(\n",
    "        output_dir=output_dir, \n",
    "        estimator=estimator,\n",
    "        random_seed=random_seed\n",
    "        )\n",
    "    \n",
    "    # experiments contains name, filename, generator, param_name \n",
    "    experiments = [\n",
    "        (\"Varying Network Size (n)\", \"results_varying_n.pkl\", generate_varying_n_params, \"n\"),\n",
    "        (\"Varying Time Points (T)\", \"results_varying_T.pkl\", generate_varying_T_params, \"T\"),\n",
    "        (\"Varying Rho Difference\", \"results_varying_rho_difference.pkl\", generate_varying_rho_difference_params, \"rho_triplet\"),\n",
    "        (\"Varying Rho Magnitude\", \"results_varying_rho_magnitude.pkl\", generate_varying_rho_magnitude_params, \"rho_triplet\"),\n",
    "        (\"Varying Stay Probabilities\", \"results_varying_stay_probs.pkl\", generate_varying_stay_probs_params, \"stay_probs\"),\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    for exp_name, filename, generator, param_name in experiments:\n",
    "        print(f\"\\n{'*='*40}\")\n",
    "        print(f\"Starting: {exp_name}\")\n",
    "        print(f\"{'*='*40}\\n\")\n",
    "        \n",
    "        run_parameter_sweep(\n",
    "            experiment_name=exp_name, \n",
    "            filename=filename, \n",
    "            parameter_generator=generator, \n",
    "            default_params=default_params, \n",
    "            varying_param_name=param_name, \n",
    "            manager=manager, \n",
    "            num_runs=num_runs\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nCompleted: {exp_name}\")\n",
    "        print(f\"{'='*80}\\n\")\n",
    "\n",
    "\n",
    "def analyze_all_results(output_dir=\"output/tmp\"):\n",
    "    \"\"\"Analyze results from all experiments.\"\"\"\n",
    "    manager = ExperimentManager(output_dir=output_dir)\n",
    "    \n",
    "    experiments = [\n",
    "        (\"Varying n\", \"results_varying_n.pkl\"),\n",
    "        (\"Varying T\", \"results_varying_T.pkl\"),\n",
    "        (\"Varying Rho Difference\", \"results_varying_rho_difference.pkl\"),\n",
    "        (\"Varying Rho Magnitude\", \"results_varying_rho_magnitude.pkl\"),\n",
    "        (\"Varying Stay Probabilities\", \"results_varying_stay_probs.pkl\"),\n",
    "    ]\n",
    "    \n",
    "    for exp_name, filename in experiments:\n",
    "        print(f\"\\n{'+-'*40}\")\n",
    "        print(f\"EXPERIMENT: {exp_name}\")\n",
    "        print(f\"{'+-'*40}\")\n",
    "        \n",
    "        results = manager.load_results(filename)\n",
    "        \n",
    "        if \"info\" in results:\n",
    "            info = results[\"info\"]\n",
    "            print(f\"Experiment details:\")\n",
    "            print(f\"  Varying parameter: {info.get('varying_parameter', 'Unknown')}\")\n",
    "            print(f\"  Completed runs: {info.get('completed_runs', 0)}/{info.get('total_runs', 0)}\")\n",
    "            print(f\"  Last updated: {info.get('last_updated', 'Unknown')}\")\n",
    "            print()\n",
    "        \n",
    "        # Create summary table\n",
    "        data = []\n",
    "        for key, result in results.items():\n",
    "            if key != \"info\" and 'error' not in result:\n",
    "                # Check if this is a multi-run result\n",
    "                if 'super_ari_std' in result:\n",
    "                    data.append({\n",
    "                        'Parameter': key,\n",
    "                        'Runs': result.get('num_runs', 1),\n",
    "                        'Super ARI': f\"{result['super_ari']:.3f} ± {result['super_ari_std']:.3f}\",\n",
    "                        'Time-avg ARI': f\"{result['mean_ari']:.3f} ± {result['mean_ari_std']:.3f}\",\n",
    "                        'Edge Density': f\"{result.get('edge_density', 0):.4f}\",\n",
    "                        'Time (s)': f\"{result['estimation_time']:.1f} ± {result['estimation_time_std']:.1f}\"\n",
    "                    })\n",
    "                else:\n",
    "                    # Legacy single-run format\n",
    "                    data.append({\n",
    "                        'Parameter': key,\n",
    "                        'Runs': 1,\n",
    "                        'Super ARI': f\"{result['super_ari']:.3f}\",\n",
    "                        'Time-avg ARI': f\"{result['mean_ari']:.3f}\",\n",
    "                        'Edge Density': f\"{result.get('edge_density', 0):.4f}\",\n",
    "                        'Time (s)': f\"{result['estimation_time']:.1f}\"\n",
    "                    })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        print(df.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada040aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all experiments for stabilized estimator \n",
    "seed = 4321 \n",
    "\n",
    "run_all_experiments(\n",
    "    num_runs=10, \n",
    "    output_dir=\"output/stabilized\",\n",
    "    estimator=dynamic_hierarchical_dcsbm_detection_stable,  \n",
    "    random_seed=seed \n",
    ")\n",
    "analyze_all_results(output_dir=\"output/stabilized\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2d0a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run all experiments for simple estimator \n",
    "\n",
    "# NB, for this particular setting and n=500, one actually can run into errors \n",
    "# caused by degenerate estimates cases.  Specifically for this, I added \n",
    "# a loop trying three times if needed.  (This is not necessary for the stabilized estimator.)\n",
    "\n",
    "par_ranges['varying_n'] = [500, 1000, 1500, 2000]\n",
    "\n",
    "par_ranges['varying_rho_magnitude'] = [\n",
    "        (0.010, 0.020, 0.080),  \n",
    "        (0.025, 0.050, 0.200),  \n",
    "        (0.050, 0.100, 0.400),  \n",
    "        (0.100, 0.200, 0.800),  \n",
    "    ]\n",
    "\n",
    "run_all_experiments(\n",
    "    num_runs=10, \n",
    "    output_dir=\"output/simple\",\n",
    "    estimator=dynamic_hierarchical_dcsbm_detection_simple, \n",
    "    random_seed=seed\n",
    ")\n",
    "analyze_all_results(output_dir=\"output/simple\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
